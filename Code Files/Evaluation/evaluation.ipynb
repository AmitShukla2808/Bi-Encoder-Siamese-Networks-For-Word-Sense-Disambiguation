{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0dec8524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\amush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\amush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import importlib\n",
    "import wic\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "importlib.reload(wic)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce8c832",
   "metadata": {},
   "source": [
    "# Gloss Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5b88cdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amush\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('kanishka/GlossBERT')\n",
    "model = BertForSequenceClassification.from_pretrained('kanishka/GlossBERT')\n",
    "model.to(device)\n",
    "\n",
    "# Custom Dataset to handle sentence pairs with labels\n",
    "# Custom Dataset\n",
    "class SentencePairDataset(Dataset):\n",
    "    def __init__(self, sentence_pairs_with_labels):\n",
    "        self.data = sentence_pairs_with_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Collate function\n",
    "def collate_fn_glossb(batch):\n",
    "    sentences1 = [item[0] for item in batch]\n",
    "    sentences2 = [item[1] for item in batch]\n",
    "    labels = torch.tensor([item[2] for item in batch], dtype=torch.long)\n",
    "    inputs = tokenizer(sentences1, sentences2, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    return inputs, labels, batch\n",
    "\n",
    "# Evaluation Function\n",
    "def classify_and_evaluate(dataloader):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    results = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, raw_batch in dataloader:\n",
    "            # Move inputs and labels to device\n",
    "            inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predicted_labels = torch.argmax(probs, dim=-1)\n",
    "\n",
    "            # Accuracy\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted_labels == labels).sum().item()\n",
    "\n",
    "            # Store results\n",
    "            for i in range(len(raw_batch)):\n",
    "                sentence1, sentence2, gold_label = raw_batch[i]\n",
    "                pred_label = predicted_labels[i].item()\n",
    "                confidence = probs[i][1].item()\n",
    "                results.append({\n",
    "                    'sentence1': sentence1,\n",
    "                    'sentence2': sentence2,\n",
    "                    'gold_label': gold_label,\n",
    "                    'predicted_label': pred_label,\n",
    "                    'confidence': confidence\n",
    "                })\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return results, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ffcc47",
   "metadata": {},
   "source": [
    "# Custom Fine-Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "268cff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class WSDSiameseDataset(Dataset):\n",
    "    def __init__(self, sentence_pairs, labels, tokenizer, max_length=128):\n",
    "        self.sentence_pairs = sentence_pairs\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sent1, sent2 = self.sentence_pairs[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        tokens_1 = self.tokenizer(sent1, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        tokens_2 = self.tokenizer(sent2, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"input_ids_1\": tokens_1[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_1\": tokens_1[\"attention_mask\"].squeeze(0),\n",
    "            \"input_ids_2\": tokens_2[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_2\": tokens_2[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.float),\n",
    "        }\n",
    "\n",
    "# Model\n",
    "class SiameseBERT(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(SiameseBERT, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, 256)\n",
    "        # Define cosine distance as a lambda function\n",
    "        self.distance = lambda x, y: F.cosine_similarity(x, y)\n",
    "\n",
    "    def get_embedding(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        proj = self.fc(cls_embedding)\n",
    "        return F.normalize(proj, p=2, dim=1)  # L2-normalize for cosine\n",
    "\n",
    "    def forward(self, input_ids_1, attention_mask_1, input_ids_2, attention_mask_2, labels):\n",
    "        emb1 = self.get_embedding(input_ids_1, attention_mask_1)\n",
    "        emb2 = self.get_embedding(input_ids_2, attention_mask_2)\n",
    "        return labels - self.distance(emb1, emb2)  # returns tensor of distances\n",
    "\n",
    "# Contrastive Loss\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, distance, label):\n",
    "        loss = (1 - label) * distance.pow(2) + label * torch.clamp(self.margin - distance, min=0.0).pow(2)\n",
    "        return loss.mean()\n",
    "\n",
    "# Collate Function\n",
    "def collate_fn_custom(batch):\n",
    "    return {\n",
    "        \"input_ids_1\": torch.stack([item[\"input_ids_1\"] for item in batch]),\n",
    "        \"attention_mask_1\": torch.stack([item[\"attention_mask_1\"] for item in batch]),\n",
    "        \"input_ids_2\": torch.stack([item[\"input_ids_2\"] for item in batch]),\n",
    "        \"attention_mask_2\": torch.stack([item[\"attention_mask_2\"] for item in batch]),\n",
    "        \"labels\": torch.stack([item[\"label\"] for item in batch]),\n",
    "    }\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "def evaluate(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            distances = model(\n",
    "                batch[\"input_ids_1\"].to(device),\n",
    "                batch[\"attention_mask_1\"].to(device),\n",
    "                batch[\"input_ids_2\"].to(device),\n",
    "                batch[\"attention_mask_2\"].to(device),\n",
    "                batch[\"labels\"].to(device)\n",
    "            )\n",
    "            loss = loss_fn(distances, batch[\"labels\"].to(device))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Prediction\n",
    "\n",
    "def predict(model, loader, device):\n",
    "    model.eval()\n",
    "    all_distances, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            distances = model(\n",
    "                batch[\"input_ids_1\"].to(device),\n",
    "                batch[\"attention_mask_1\"].to(device),\n",
    "                batch[\"input_ids_2\"].to(device),\n",
    "                batch[\"attention_mask_2\"].to(device),\n",
    "                batch[\"labels\"].to(device)\n",
    "            )\n",
    "            all_distances.extend(distances.cpu().numpy())\n",
    "            all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "    return np.array(all_distances), np.array(all_labels)\n",
    "\n",
    "# Metrics\n",
    "\n",
    "def compute_accuracy_f1(distances, labels, threshold=0.0):\n",
    "    preds = (distances > threshold).astype(int)\n",
    "    print(labels)\n",
    "    print(preds)\n",
    "    accuracy = (preds == labels).mean()\n",
    "    f1 = f1_score(labels, preds)\n",
    "    return accuracy, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bc9e5d",
   "metadata": {},
   "source": [
    "# TinyBert Fine-Tuned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "47aab1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amush\\AppData\\Local\\Temp\\ipykernel_24248\\1838878308.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_tiny.bert.load_state_dict(torch.load(r\"C:\\Users\\amush\\INLP_Project\\Finetuning\\fine_tuned_tiny.pth\"))\n"
     ]
    }
   ],
   "source": [
    "model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
    "tokenizer_tiny = AutoTokenizer.from_pretrained(model_name)\n",
    "model_tiny = SiameseBERT(model_name=model_name)\n",
    "model_tiny.bert.load_state_dict(torch.load(r\"C:\\Users\\amush\\INLP_Project\\Finetuning\\fine_tuned_tiny.pth\"))\n",
    "loss_fn = ContrastiveLoss(margin=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9ae0d4",
   "metadata": {},
   "source": [
    "# DistilBert Fine-Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ec62fdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amush\\AppData\\Local\\Temp\\ipykernel_24248\\1976440594.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_distil.bert.load_state_dict(torch.load(r\"C:\\Users\\amush\\INLP_Project\\Finetuning\\fine_tuned_distill.pth\"))\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer_distil = AutoTokenizer.from_pretrained(model_name)\n",
    "model_distil = SiameseBERT(model_name=model_name)\n",
    "model_distil.bert.load_state_dict(torch.load(r\"C:\\Users\\amush\\INLP_Project\\Finetuning\\fine_tuned_distill.pth\"))\n",
    "loss_fn = ContrastiveLoss(margin=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1264689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f7ba41b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a02eab",
   "metadata": {},
   "source": [
    "# SemEval 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "73c287f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "      <th>lemma</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This document is a summary of the European Pub...</td>\n",
       "      <td>This document is a summary of the European Pub...</td>\n",
       "      <td>document</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It explains how the Committee for Medicinal Pr...</td>\n",
       "      <td>It explains how the Committee for Medicinal Pr...</td>\n",
       "      <td>explain</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you want more information on the basis of t...</td>\n",
       "      <td>If we want to understand how it works , the be...</td>\n",
       "      <td>want</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you want more information on the basis of t...</td>\n",
       "      <td>If you want to use a typical f(x) function it ...</td>\n",
       "      <td>want</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If we want to understand how it works , the be...</td>\n",
       "      <td>If you want to use a typical f(x) function it ...</td>\n",
       "      <td>want</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>The Foundation organised , together with the E...</td>\n",
       "      <td>The Foundation recently published a comparativ...</td>\n",
       "      <td>foundation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>The Foundation organised , together with the E...</td>\n",
       "      <td>The Foundation aims to document the characteri...</td>\n",
       "      <td>foundation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>The Foundation recently published a comparativ...</td>\n",
       "      <td>The Foundation aims to document the characteri...</td>\n",
       "      <td>foundation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Case Studies Each national report contains a p...</td>\n",
       "      <td>These case studies analysed the background of ...</td>\n",
       "      <td>case_study</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>The tablets are pale-orange and have a score l...</td>\n",
       "      <td>Each tablet is marked with the Pfizer logo on ...</td>\n",
       "      <td>tablet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sent1  \\\n",
       "0    This document is a summary of the European Pub...   \n",
       "1    It explains how the Committee for Medicinal Pr...   \n",
       "2    If you want more information on the basis of t...   \n",
       "3    If you want more information on the basis of t...   \n",
       "4    If we want to understand how it works , the be...   \n",
       "..                                                 ...   \n",
       "102  The Foundation organised , together with the E...   \n",
       "103  The Foundation organised , together with the E...   \n",
       "104  The Foundation recently published a comparativ...   \n",
       "105  Case Studies Each national report contains a p...   \n",
       "106  The tablets are pale-orange and have a score l...   \n",
       "\n",
       "                                                 sent2       lemma  \\\n",
       "0    This document is a summary of the European Pub...    document   \n",
       "1    It explains how the Committee for Medicinal Pr...     explain   \n",
       "2    If we want to understand how it works , the be...        want   \n",
       "3    If you want to use a typical f(x) function it ...        want   \n",
       "4    If you want to use a typical f(x) function it ...        want   \n",
       "..                                                 ...         ...   \n",
       "102  The Foundation recently published a comparativ...  foundation   \n",
       "103  The Foundation aims to document the characteri...  foundation   \n",
       "104  The Foundation aims to document the characteri...  foundation   \n",
       "105  These case studies analysed the background of ...  case_study   \n",
       "106  Each tablet is marked with the Pfizer logo on ...      tablet   \n",
       "\n",
       "     ground_truth  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "..            ...  \n",
       "102             0  \n",
       "103             0  \n",
       "104             0  \n",
       "105             0  \n",
       "106             0  \n",
       "\n",
       "[107 rows x 4 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem15 = pd.read_csv(r'C:\\Users\\amush\\INLP_Project\\Finetuning\\semeval2015.csv')\n",
    "sem15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241841a0",
   "metadata": {},
   "source": [
    "### TinyBert Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122dd741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6336020231246948"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_pairs = list(zip(sem15[\"sent1\"], sem15[\"sent2\"]))\n",
    "labels = list(sem15[\"ground_truth\"])\n",
    "model_tiny.to(device)\n",
    "test_dataset = WSDSiameseDataset(sentence_pairs, labels, tokenizer_tiny)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn_custom)\n",
    "evaluate(model_tiny, test_loader, loss_fn, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58791e0a",
   "metadata": {},
   "source": [
    "### DistilBert Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f99b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6077485382556915"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_pairs = list(zip(sem15[\"sent1\"], sem15[\"sent2\"]))\n",
    "labels = list(sem15[\"ground_truth\"])\n",
    "model_distil.to(device)\n",
    "test_dataset = WSDSiameseDataset(sentence_pairs, labels, tokenizer_distil)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn_custom)\n",
    "evaluate(model_tiny, test_loader, loss_fn, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d8237b",
   "metadata": {},
   "source": [
    "### GlossBert Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "76a42e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5794392523364486"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glossbert_data = [(sem15['sent1'][i], sem15['sent2'][i], sem15['ground_truth'][i]) for i in range(len(sem15))]\n",
    "dataset = SentencePairDataset(glossbert_data)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=False, collate_fn=collate_fn_glossb)\n",
    "model.to(device)\n",
    "# Run classification and compute accuracy\n",
    "results, accuracy = classify_and_evaluate(dataloader)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc868f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd1e7d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9567839d",
   "metadata": {},
   "source": [
    "# SemEval 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "aa02e262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "      <th>lemma</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The U.N.-sponsored climate conference -- chara...</td>\n",
       "      <td>Artur Runge-Metzger , who heads international ...</td>\n",
       "      <td>climate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It gives a lot of flexibility to the process ,...</td>\n",
       "      <td>There is a lot of consensus between the Left a...</td>\n",
       "      <td>lot</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Together , the countries would cut emissions b...</td>\n",
       "      <td>Some of the countries most vulnerable to the i...</td>\n",
       "      <td>country</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U.S. special climate envoy Todd Stern rejected...</td>\n",
       "      <td>U.S. firms were in some cases at a disadvantag...</td>\n",
       "      <td>u.s.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. special climate envoy Todd Stern rejected...</td>\n",
       "      <td>Major U.S. firms such as Chevron and ConocoPhi...</td>\n",
       "      <td>u.s.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>The only difference , the degree of generosity</td>\n",
       "      <td>The only difference resides in the degree of g...</td>\n",
       "      <td>difference</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>In 2005 , the fear of invasion of the national...</td>\n",
       "      <td>Fears about the impact of immigrants are based...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>The National Credit Union Administration ( NCU...</td>\n",
       "      <td>The regulator had also approached other major ...</td>\n",
       "      <td>regulator</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>The National Credit Union Administration ( NCU...</td>\n",
       "      <td>The regulator is demanding that the Frankfurt-...</td>\n",
       "      <td>regulator</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>The regulator had also approached other major ...</td>\n",
       "      <td>The regulator is demanding that the Frankfurt-...</td>\n",
       "      <td>regulator</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sent1  \\\n",
       "0    The U.N.-sponsored climate conference -- chara...   \n",
       "1    It gives a lot of flexibility to the process ,...   \n",
       "2    Together , the countries would cut emissions b...   \n",
       "3    U.S. special climate envoy Todd Stern rejected...   \n",
       "4    U.S. special climate envoy Todd Stern rejected...   \n",
       "..                                                 ...   \n",
       "145     The only difference , the degree of generosity   \n",
       "146  In 2005 , the fear of invasion of the national...   \n",
       "147  The National Credit Union Administration ( NCU...   \n",
       "148  The National Credit Union Administration ( NCU...   \n",
       "149  The regulator had also approached other major ...   \n",
       "\n",
       "                                                 sent2       lemma  \\\n",
       "0    Artur Runge-Metzger , who heads international ...     climate   \n",
       "1    There is a lot of consensus between the Left a...         lot   \n",
       "2    Some of the countries most vulnerable to the i...     country   \n",
       "3    U.S. firms were in some cases at a disadvantag...        u.s.   \n",
       "4    Major U.S. firms such as Chevron and ConocoPhi...        u.s.   \n",
       "..                                                 ...         ...   \n",
       "145  The only difference resides in the degree of g...  difference   \n",
       "146  Fears about the impact of immigrants are based...        fear   \n",
       "147  The regulator had also approached other major ...   regulator   \n",
       "148  The regulator is demanding that the Frankfurt-...   regulator   \n",
       "149  The regulator is demanding that the Frankfurt-...   regulator   \n",
       "\n",
       "     ground_truth  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "..            ...  \n",
       "145             0  \n",
       "146             0  \n",
       "147             0  \n",
       "148             0  \n",
       "149             0  \n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem13 = pd.read_csv(r'C:\\Users\\amush\\INLP_Project\\Finetuning\\semeval2013.csv')\n",
    "sem13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf05388d",
   "metadata": {},
   "source": [
    "### TinyBert Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9217777b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7464426159858704"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_pairs = list(zip(sem13[\"sent1\"], sem13[\"sent2\"]))\n",
    "labels = list(sem13[\"ground_truth\"])\n",
    "model_tiny.to(device)\n",
    "test_dataset = WSDSiameseDataset(sentence_pairs, labels, tokenizer_tiny)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn_custom)\n",
    "evaluate(model_tiny, test_loader, loss_fn, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d3ee9",
   "metadata": {},
   "source": [
    "### DistilBert Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5e64be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7541502594947815"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_pairs = list(zip(sem13[\"sent1\"], sem13[\"sent2\"]))\n",
    "labels = list(sem13[\"ground_truth\"])\n",
    "model_distil.to(device)\n",
    "test_dataset = WSDSiameseDataset(sentence_pairs, labels, tokenizer_distil)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn_custom)\n",
    "evaluate(model_tiny, test_loader, loss_fn, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52297706",
   "metadata": {},
   "source": [
    "### GlossBert Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3b995dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5533333333333333"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glossbert_data = [(sem13['sent1'][i], sem13['sent2'][i], sem13['ground_truth'][i]) for i in range(len(sem13))]\n",
    "dataset = SentencePairDataset(glossbert_data)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False, collate_fn=collate_fn_glossb)\n",
    "model.to(device)\n",
    "# Run classification and compute accuracy\n",
    "results, accuracy = classify_and_evaluate(dataloader)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce20eef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60edb8ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b3a6a6a",
   "metadata": {},
   "source": [
    "# RAW-C (Related Words in Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "7fa5e8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>act</td>\n",
       "      <td>It was a desperate act.</td>\n",
       "      <td>It was a magic act.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act</td>\n",
       "      <td>It was a desperate act.</td>\n",
       "      <td>It was a comedic act.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>act</td>\n",
       "      <td>It was a humane act.</td>\n",
       "      <td>It was a magic act.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>act</td>\n",
       "      <td>It was a humane act.</td>\n",
       "      <td>It was a comedic act.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>act</td>\n",
       "      <td>It was a desperate act.</td>\n",
       "      <td>It was a humane act.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>yard</td>\n",
       "      <td>It was five yards.</td>\n",
       "      <td>They were cluttered yards.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>yard</td>\n",
       "      <td>It was ten yards.</td>\n",
       "      <td>They were big yards.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>yard</td>\n",
       "      <td>It was ten yards.</td>\n",
       "      <td>They were cluttered yards.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>yard</td>\n",
       "      <td>It was five yards.</td>\n",
       "      <td>It was ten yards.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>yard</td>\n",
       "      <td>They were big yards.</td>\n",
       "      <td>They were cluttered yards.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>672 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    lemma                    sent1                       sent2  ground_truth\n",
       "0     act  It was a desperate act.         It was a magic act.             0\n",
       "1     act  It was a desperate act.       It was a comedic act.             0\n",
       "2     act     It was a humane act.         It was a magic act.             0\n",
       "3     act     It was a humane act.       It was a comedic act.             0\n",
       "4     act  It was a desperate act.        It was a humane act.             1\n",
       "..    ...                      ...                         ...           ...\n",
       "667  yard       It was five yards.  They were cluttered yards.             0\n",
       "668  yard        It was ten yards.        They were big yards.             0\n",
       "669  yard        It was ten yards.  They were cluttered yards.             0\n",
       "670  yard       It was five yards.           It was ten yards.             1\n",
       "671  yard     They were big yards.  They were cluttered yards.             1\n",
       "\n",
       "[672 rows x 4 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawc = pd.read_csv(r'C:\\Users\\amush\\INLP_Project\\Finetuning\\raw-c.csv')\n",
    "rawc = rawc[['word', 'sentence1', 'sentence2', 'same']]\n",
    "rawc = rawc.rename(columns={'word':'lemma', 'sentence1': 'sent1', 'sentence2': 'sent2', 'same':'ground_truth'})\n",
    "rawc['ground_truth']  = rawc['ground_truth'].apply(lambda x : 0 if x == False else 1)\n",
    "rawc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5431360c",
   "metadata": {},
   "source": [
    "### TinyBert Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "01cf5150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7395407330422175"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_pairs = list(zip(rawc[\"sent1\"], rawc[\"sent2\"]))\n",
    "labels = list(rawc[\"ground_truth\"])\n",
    "model_tiny.to(device)\n",
    "test_dataset = WSDSiameseDataset(sentence_pairs, labels, tokenizer_tiny)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn_custom)\n",
    "evaluate(model_tiny, test_loader, loss_fn, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedf646a",
   "metadata": {},
   "source": [
    "### DistilBert Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "72232519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7395407330422175"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_pairs = list(zip(rawc[\"sent1\"], rawc[\"sent2\"]))\n",
    "labels = list(rawc[\"ground_truth\"])\n",
    "model_distil.to(device)\n",
    "test_dataset = WSDSiameseDataset(sentence_pairs, labels, tokenizer_distil)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn_custom)\n",
    "evaluate(model_tiny, test_loader, loss_fn, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c680df",
   "metadata": {},
   "source": [
    "### GlossBert Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "047b4226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6190476190476191"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glossbert_data = [(rawc['sent1'][i], rawc['sent2'][i], rawc['ground_truth'][i]) for i in range(len(rawc))]\n",
    "dataset = SentencePairDataset(glossbert_data)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False, collate_fn=collate_fn_glossb)\n",
    "model.to(device)\n",
    "# Run classification and compute accuracy\n",
    "results, accuracy = classify_and_evaluate(dataloader)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df372889",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
