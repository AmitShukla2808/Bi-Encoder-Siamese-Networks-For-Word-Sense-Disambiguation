{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11660482,"sourceType":"datasetVersion","datasetId":7317608},{"sourceId":370990,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":307098,"modelId":327576}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer, BertModel\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn.model_selection import train_test_split\n# import wic\nimport warnings\nfrom tqdm import tqdm\nfrom torch.cuda.amp import GradScaler, autocast\nfrom datasets import Dataset\nimport math\nimport pandas as pd\nimport numpy as np\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:05:26.366376Z","iopub.execute_input":"2025-05-03T10:05:26.367220Z","iopub.status.idle":"2025-05-03T10:05:26.373296Z","shell.execute_reply.started":"2025-05-03T10:05:26.367185Z","shell.execute_reply":"2025-05-03T10:05:26.372382Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass WSDTripletDataset(Dataset):\n    def __init__(self, hf_dataset, tokenizer, max_length):\n        self.dataset = hf_dataset\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        data = self.dataset.iloc[idx]  # Use iloc for pandas DataFrame\n\n        def tokenize(text):\n            tokens = self.tokenizer(\n                text,\n                padding=\"max_length\",\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors=\"pt\"\n            )\n            return {\n                'input_ids': tokens['input_ids'].squeeze(0),\n                'attention_mask': tokens['attention_mask'].squeeze(0)\n            }\n\n        return {\n            'anchor': tokenize(data['anchor']),\n            'positive': tokenize(data['positive']),\n            'negative': tokenize(data['negative']),\n            'target_word': data['target_word']\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:05:26.841807Z","iopub.execute_input":"2025-05-03T10:05:26.842106Z","iopub.status.idle":"2025-05-03T10:05:26.847788Z","shell.execute_reply.started":"2025-05-03T10:05:26.842085Z","shell.execute_reply":"2025-05-03T10:05:26.847153Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from transformers import AutoModel\n\nclass TripletBERT(nn.Module):\n    def __init__(self, model_name):\n        super(TripletBERT, self).__init__()\n        self.bert = AutoModel.from_pretrained(model_name)  # Changed to AutoModel\n        self.hidden_size = self.bert.config.hidden_size\n        self.fc = nn.Linear(self.hidden_size, 256)\n\n    def get_embedding(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        hidden_states = outputs.last_hidden_state\n        cls_embedding = hidden_states[:, 0, :]\n        proj = self.fc(cls_embedding)\n        proj = torch.nn.functional.normalize(proj, p=2, dim=1)  # L2 normalize\n        return proj\n\n\n    def forward(self, anchor, positive, negative):\n        anchor_embed = self.get_embedding(anchor[\"input_ids\"], anchor[\"attention_mask\"])\n        positive_embed = self.get_embedding(positive[\"input_ids\"], positive[\"attention_mask\"])\n        negative_embed = self.get_embedding(negative[\"input_ids\"], negative[\"attention_mask\"])\n        return anchor_embed, positive_embed, negative_embed\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:05:27.879096Z","iopub.execute_input":"2025-05-03T10:05:27.879607Z","iopub.status.idle":"2025-05-03T10:05:27.885631Z","shell.execute_reply.started":"2025-05-03T10:05:27.879585Z","shell.execute_reply":"2025-05-03T10:05:27.884870Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class TripletLoss(nn.Module):\n    def __init__(self, margin=0.7):\n        super(TripletLoss, self).__init__()\n        self.margin = margin\n        self.cosine = nn.CosineSimilarity(dim=1)\n        self.distance = lambda x, y: 1 - self.cosine(x, y)\n\n    def forward(self, anchor, positive, negative):\n        d_pos = self.distance(anchor, positive)\n        d_neg = self.distance(anchor, negative)\n        loss = torch.clamp(d_pos - d_neg + self.margin, min=0.0)\n        return loss.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:37:01.800823Z","iopub.execute_input":"2025-05-03T10:37:01.801213Z","iopub.status.idle":"2025-05-03T10:37:01.806355Z","shell.execute_reply.started":"2025-05-03T10:37:01.801193Z","shell.execute_reply":"2025-05-03T10:37:01.805573Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def collate_fn(batch):\n    return {\n        'anchor': {\n            'input_ids': torch.stack([item['anchor']['input_ids'] for item in batch]),\n            'attention_mask': torch.stack([item['anchor']['attention_mask'] for item in batch])\n        },\n        'positive': {\n            'input_ids': torch.stack([item['positive']['input_ids'] for item in batch]),\n            'attention_mask': torch.stack([item['positive']['attention_mask'] for item in batch])\n        },\n        'negative': {\n            'input_ids': torch.stack([item['negative']['input_ids'] for item in batch]),\n            'attention_mask': torch.stack([item['negative']['attention_mask'] for item in batch])\n        },\n        'target_word': [item['target_word'] for item in batch]\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:37:09.124216Z","iopub.execute_input":"2025-05-03T10:37:09.124745Z","iopub.status.idle":"2025-05-03T10:37:09.129743Z","shell.execute_reply.started":"2025-05-03T10:37:09.124723Z","shell.execute_reply":"2025-05-03T10:37:09.128940Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import torch\nimport math\nfrom tqdm import tqdm\nfrom torch.cuda.amp import autocast, GradScaler\n\ndef train_triplet(model, trainloader, testloader, optimizer, loss_fn, device, num_epochs=5):\n    model.to(device)\n    scaler = GradScaler()\n    best_loss = math.inf\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0.0\n        progress_bar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n\n        for batch in progress_bar:\n            optimizer.zero_grad()\n\n            # Move to device\n            anchor_ids = batch[\"anchor\"][\"input_ids\"].to(device)\n            anchor_mask = batch[\"anchor\"][\"attention_mask\"].to(device)\n            pos_ids = batch[\"positive\"][\"input_ids\"].to(device)\n            pos_mask = batch[\"positive\"][\"attention_mask\"].to(device)\n            neg_ids = batch[\"negative\"][\"input_ids\"].to(device)\n            neg_mask = batch[\"negative\"][\"attention_mask\"].to(device)\n\n            with autocast():\n                anchor_embed = model.get_embedding(anchor_ids, anchor_mask)\n                pos_embed = model.get_embedding(pos_ids, pos_mask)\n                neg_embed = model.get_embedding(neg_ids, neg_mask)\n\n                loss = loss_fn(anchor_embed, pos_embed, neg_embed)\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            total_loss += loss.item()\n            progress_bar.set_postfix({\"Train Loss\": loss.item()})\n\n        avg_train_loss = total_loss / len(trainloader)\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for batch in testloader:\n                anchor_ids = batch[\"anchor\"][\"input_ids\"].to(device)\n                anchor_mask = batch[\"anchor\"][\"attention_mask\"].to(device)\n                pos_ids = batch[\"positive\"][\"input_ids\"].to(device)\n                pos_mask = batch[\"positive\"][\"attention_mask\"].to(device)\n                neg_ids = batch[\"negative\"][\"input_ids\"].to(device)\n                neg_mask = batch[\"negative\"][\"attention_mask\"].to(device)\n\n                anchor_embed = model.get_embedding(anchor_ids, anchor_mask)\n                pos_embed = model.get_embedding(pos_ids, pos_mask)\n                neg_embed = model.get_embedding(neg_ids, neg_mask)\n\n                loss = loss_fn(anchor_embed, pos_embed, neg_embed)\n                val_loss += loss.item()\n\n        avg_val_loss = val_loss / len(testloader)\n\n        # Save the best model based on validation loss\n        if avg_val_loss < best_loss:\n            best_loss = avg_val_loss\n            torch.save(model.bert.state_dict(), \"shared_weights_triplet_distil.pth\")\n\n        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:37:09.484544Z","iopub.execute_input":"2025-05-03T10:37:09.484979Z","iopub.status.idle":"2025-05-03T10:37:09.494902Z","shell.execute_reply.started":"2025-05-03T10:37:09.484958Z","shell.execute_reply":"2025-05-03T10:37:09.494309Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Load and preprocess the triplet dataset\ndata_triplet = pd.read_csv(\"/kaggle/input/context-positive-negative/context_gloss_triplets_all_negatives.csv\")\ndata_triplet = data_triplet.rename(columns={\n    'Context Sentence (Anchor)': 'anchor',\n    'Gloss Definition (Positive)': 'positive',\n    'Gloss Definition (Negative)': 'negative'\n})\ndata_triplet['target_word'] = data_triplet['positive'].str.extract(r'^(\\w+)\\s*:')\n\ntriplet_df = data_triplet.dropna(subset=['anchor', 'positive', 'negative', 'target_word'])\ntriplet_df = triplet_df.astype(str)\n\ntrain_data, test_data = train_test_split(triplet_df, test_size=0.2, random_state=42)\ntrain_data = train_data.reset_index(drop=True)\ntest_data = test_data.reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:37:10.701244Z","iopub.execute_input":"2025-05-03T10:37:10.701521Z","iopub.status.idle":"2025-05-03T10:37:11.002654Z","shell.execute_reply.started":"2025-05-03T10:37:10.701490Z","shell.execute_reply":"2025-05-03T10:37:11.002081Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Model setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_name = \"distilbert-base-uncased\"\nmodel = TripletBERT(model_name=model_name)\nmodel.bert.load_state_dict(torch.load(\"/kaggle/input/triplet_pretrained_distilbert/pytorch/default/1/shared_weights_gloss_hypernym_distil.pth\", map_location=device))\n\n# Updated optimizer and loss\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n\nloss_fn = TripletLoss(margin=2.0)  # Lower margin helps smoother convergence","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:37:19.943946Z","iopub.execute_input":"2025-05-03T10:37:19.944246Z","iopub.status.idle":"2025-05-03T10:37:20.350330Z","shell.execute_reply.started":"2025-05-03T10:37:19.944226Z","shell.execute_reply":"2025-05-03T10:37:20.349592Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3350551280.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.bert.load_state_dict(torch.load(\"/kaggle/input/gloss-hypernymy-tinybert-pretrained/pytorch/default/1/shared_weights_gloss_hypernym_tiny.pth\", map_location=device))\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# Tokenizer and Dataloaders\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntrain_dataset = WSDTripletDataset(train_data, tokenizer, max_length=128)\ntest_dataset = WSDTripletDataset(test_data, tokenizer, max_length=128)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:37:21.255822Z","iopub.execute_input":"2025-05-03T10:37:21.256518Z","iopub.status.idle":"2025-05-03T10:37:21.661184Z","shell.execute_reply.started":"2025-05-03T10:37:21.256488Z","shell.execute_reply":"2025-05-03T10:37:21.660627Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# # Model setup\n# model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\n# model = TripletBERT(model_name=model_name)\n# optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n# loss_fn = TripletLoss(margin=1)\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:37:23.834238Z","iopub.execute_input":"2025-05-03T10:37:23.834529Z","iopub.status.idle":"2025-05-03T10:37:23.838265Z","shell.execute_reply.started":"2025-05-03T10:37:23.834507Z","shell.execute_reply":"2025-05-03T10:37:23.837406Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Train the model\ntrain_triplet(\n    model=model,\n    trainloader=train_loader,\n    testloader = test_loader,\n    optimizer=optimizer,\n    loss_fn=loss_fn,\n    device=device,\n    num_epochs=20\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:37:24.175491Z","iopub.execute_input":"2025-05-03T10:37:24.175763Z","iopub.status.idle":"2025-05-03T10:57:51.175276Z","shell.execute_reply.started":"2025-05-03T10:37:24.175744Z","shell.execute_reply":"2025-05-03T10:57:51.174481Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3848801276.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\nEpoch 1/5:   0%|          | 0/1740 [00:00<?, ?it/s]/tmp/ipykernel_31/3848801276.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nEpoch 1/5: 100%|██████████| 1740/1740 [03:40<00:00,  7.88it/s, Train Loss=0.184]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5 - Train Loss: 0.4879 - Val Loss: 0.3305\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 1740/1740 [03:40<00:00,  7.87it/s, Train Loss=0.0989]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5 - Train Loss: 0.3024 - Val Loss: 0.3030\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 1740/1740 [03:40<00:00,  7.89it/s, Train Loss=0.606]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5 - Train Loss: 0.2635 - Val Loss: 0.2938\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 1740/1740 [03:41<00:00,  7.84it/s, Train Loss=0.0834]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5 - Train Loss: 0.2454 - Val Loss: 0.2588\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 1740/1740 [03:41<00:00,  7.86it/s, Train Loss=0.012]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5 - Train Loss: 0.2293 - Val Loss: 0.2457\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"# Evaluation\n","metadata":{}},{"cell_type":"code","source":"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# # Reinitialize model and load state_dict\n# model = TripletBERT(model_name=model_name).to(device)\n# model.load_state_dict(torch.load(\"/kaggle/input/triplet_tinybert_30_0.7m/pytorch/default/1/triplet_tinybert_25_0.7M.pth\", map_location=device))\n# model.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:58:59.466317Z","iopub.execute_input":"2025-05-03T10:58:59.466601Z","iopub.status.idle":"2025-05-03T10:58:59.470067Z","shell.execute_reply.started":"2025-05-03T10:58:59.466581Z","shell.execute_reply":"2025-05-03T10:58:59.469374Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Example anchor and positive sentences\nanchor_sentence = \"The cat is on the mat.\"\npositive_sentence = \"A cat is sitting on a mat.\"\n\n# Tokenize the sentences\nanchor_tokens = tokenizer(anchor_sentence, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\npositive_tokens = tokenizer(positive_sentence, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n\n# Move tokens to the device\nanchor_tokens = {k: v.to(device) for k, v in anchor_tokens.items()}\npositive_tokens = {k: v.to(device) for k, v in positive_tokens.items()}\n\n# Get the embeddings from the model\nmodel.eval()  # Switch to evaluation mode\nwith torch.no_grad():\n    anchor_embed = model.get_embedding(anchor_tokens['input_ids'], anchor_tokens['attention_mask'])\n    positive_embed = model.get_embedding(positive_tokens['input_ids'], positive_tokens['attention_mask'])\n\n# Convert embeddings to numpy for cosine similarity calculation\nanchor_embed = anchor_embed.cpu().numpy()\npositive_embed = positive_embed.cpu().numpy()\n\n# Compute cosine similarity\ncosine_sim = cosine_similarity(anchor_embed, positive_embed)\nprint(f\"Cosine Similarity between anchor and positive sentence: {cosine_sim[0][0]:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:59:01.882359Z","iopub.execute_input":"2025-05-03T10:59:01.882902Z","iopub.status.idle":"2025-05-03T10:59:01.900623Z","shell.execute_reply.started":"2025-05-03T10:59:01.882882Z","shell.execute_reply":"2025-05-03T10:59:01.899984Z"}},"outputs":[{"name":"stdout","text":"Cosine Similarity between anchor and positive sentence: 0.9981\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# Example negative sentence\nnegative_sentence = \"A dog is barking outside.\"\n\n# Tokenize the negative sentence\nnegative_tokens = tokenizer(negative_sentence, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\nnegative_tokens = {k: v.to(device) for k, v in negative_tokens.items()}\n\n# Get the negative embedding from the model\nwith torch.no_grad():\n    negative_embed = model.get_embedding(negative_tokens['input_ids'], negative_tokens['attention_mask'])\n\n# Convert to numpy for cosine similarity\nnegative_embed = negative_embed.cpu().numpy()\n\n# Compute cosine similarity for both anchor-positive and anchor-negative\ncosine_sim_pos = cosine_similarity(anchor_embed, positive_embed)\ncosine_sim_neg = cosine_similarity(anchor_embed, negative_embed)\n\nprint(f\"Cosine Similarity between anchor and positive sentence: {cosine_sim_pos[0][0]:.4f}\")\nprint(f\"Cosine Similarity between anchor and negative sentence: {cosine_sim_neg[0][0]:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:59:04.721380Z","iopub.execute_input":"2025-05-03T10:59:04.722086Z","iopub.status.idle":"2025-05-03T10:59:04.733591Z","shell.execute_reply.started":"2025-05-03T10:59:04.722060Z","shell.execute_reply":"2025-05-03T10:59:04.732927Z"}},"outputs":[{"name":"stdout","text":"Cosine Similarity between anchor and positive sentence: 0.9981\nCosine Similarity between anchor and negative sentence: 0.9910\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}