{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11648774,"sourceType":"datasetVersion","datasetId":7310032}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports\nimport os\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\nfrom torch.cuda.amp import autocast, GradScaler\nfrom transformers import AutoModel, AutoTokenizer\nfrom sklearn.metrics import f1_score\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-02T14:45:05.597159Z","iopub.execute_input":"2025-05-02T14:45:05.597366Z","iopub.status.idle":"2025-05-02T14:45:09.351040Z","shell.execute_reply.started":"2025-05-02T14:45:05.597340Z","shell.execute_reply":"2025-05-02T14:45:09.350235Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Dataset\nclass WSDSiameseDataset(Dataset):\n    def __init__(self, sentence_pairs, labels, tokenizer, max_length=128):\n        self.sentence_pairs = sentence_pairs\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.sentence_pairs)\n\n    def __getitem__(self, idx):\n        sent1, sent2 = self.sentence_pairs[idx]\n        label = self.labels[idx]\n\n        tokens_1 = self.tokenizer(sent1, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n        tokens_2 = self.tokenizer(sent2, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n\n        return {\n            \"input_ids_1\": tokens_1[\"input_ids\"].squeeze(0),\n            \"attention_mask_1\": tokens_1[\"attention_mask\"].squeeze(0),\n            \"input_ids_2\": tokens_2[\"input_ids\"].squeeze(0),\n            \"attention_mask_2\": tokens_2[\"attention_mask\"].squeeze(0),\n            \"label\": torch.tensor(label, dtype=torch.float),\n        }\n\n# Model\nclass SiameseBERT(nn.Module):\n    def __init__(self, model_name):\n        super(SiameseBERT, self).__init__()\n        self.bert = AutoModel.from_pretrained(model_name)\n        self.fc = nn.Linear(self.bert.config.hidden_size, 256)\n        # Define cosine distance as a lambda function\n        self.distance = lambda x, y: F.cosine_similarity(x, y)\n\n    def get_embedding(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n        proj = self.fc(cls_embedding)\n        return F.normalize(proj, p=2, dim=1)  # L2-normalize for cosine\n\n    def forward(self, input_ids_1, attention_mask_1, input_ids_2, attention_mask_2, labels):\n        emb1 = self.get_embedding(input_ids_1, attention_mask_1)\n        emb2 = self.get_embedding(input_ids_2, attention_mask_2)\n        return labels - self.distance(emb1, emb2)  # returns tensor of distances\n\n# Contrastive Loss\nclass ContrastiveLoss(nn.Module):\n    def __init__(self, margin=1.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, distance, label):\n        loss = (1 - label) * distance.pow(2) + label * torch.clamp(self.margin - distance, min=0.0).pow(2)\n        return loss.mean()\n\n# Collate Function\ndef collate_fn(batch):\n    return {\n        \"input_ids_1\": torch.stack([item[\"input_ids_1\"] for item in batch]),\n        \"attention_mask_1\": torch.stack([item[\"attention_mask_1\"] for item in batch]),\n        \"input_ids_2\": torch.stack([item[\"input_ids_2\"] for item in batch]),\n        \"attention_mask_2\": torch.stack([item[\"attention_mask_2\"] for item in batch]),\n        \"labels\": torch.stack([item[\"label\"] for item in batch]),\n    }\n\n# Evaluation\n\ndef evaluate(model, loader, loss_fn, device):\n    model.eval()\n    total_loss = 0.0\n    with torch.no_grad():\n        for batch in loader:\n            distances = model(\n                batch[\"input_ids_1\"].to(device),\n                batch[\"attention_mask_1\"].to(device),\n                batch[\"input_ids_2\"].to(device),\n                batch[\"attention_mask_2\"].to(device),\n                batch[\"labels\"].to(device)\n            )\n            loss = loss_fn(distances, batch[\"labels\"].to(device))\n            total_loss += loss.item()\n    return total_loss / len(loader)\n\n# Prediction\n\ndef predict(model, loader, device):\n    model.eval()\n    all_distances, all_labels = [], []\n    with torch.no_grad():\n        for batch in loader:\n            distances = model(\n                batch[\"input_ids_1\"].to(device),\n                batch[\"attention_mask_1\"].to(device),\n                batch[\"input_ids_2\"].to(device),\n                batch[\"attention_mask_2\"].to(device),\n                batch[\"labels\"].to(device)\n            )\n            all_distances.extend(distances.cpu().numpy())\n            all_labels.extend(batch[\"labels\"].cpu().numpy())\n    return np.array(all_distances), np.array(all_labels)\n\n# Metrics\n\ndef compute_accuracy_f1(distances, labels, threshold=0.0):\n    preds = (distances > threshold).astype(int)\n    print(labels)\n    print(preds)\n    accuracy = (preds == labels).mean()\n    f1 = f1_score(labels, preds)\n    return accuracy, f1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T14:45:09.351819Z","iopub.execute_input":"2025-05-02T14:45:09.352184Z","iopub.status.idle":"2025-05-02T14:45:09.367229Z","shell.execute_reply.started":"2025-05-02T14:45:09.352165Z","shell.execute_reply":"2025-05-02T14:45:09.366529Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = SiameseBERT(model_name=model_name)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\nloss_fn = ContrastiveLoss(margin=0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T14:45:09.367816Z","iopub.execute_input":"2025-05-02T14:45:09.368071Z","iopub.status.idle":"2025-05-02T14:45:15.637477Z","shell.execute_reply.started":"2025-05-02T14:45:09.368047Z","shell.execute_reply":"2025-05-02T14:45:15.636913Z"}},"outputs":[{"name":"stderr","text":"2025-05-02 14:45:12.655652: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746197112.678141     109 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746197112.684885     109 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Manually define a balanced list of 4 test samples (2 label 0 and 2 label 1)\nbalanced_test_samples = [\n    {\n        \"Sentence 1\": \"The lawyer presented his case in court.\",\n        \"Sentence 2\": \"The detective reviewed the case again to find new clues.\",\n        \"Lemma\": \"case\",\n        \"Label\": 1\n    },\n    {\n        \"Sentence 1\": \"He closed the deal with a handshake.\",\n        \"Sentence 2\": \"She found a great deal on shoes online.\",\n        \"Lemma\": \"deal\",\n        \"Label\": 1\n    },\n    {\n        \"Sentence 1\": \"He went to the bank to deposit his paycheck.\",\n        \"Sentence 2\": \"She visited the bank with her friend yesterday to withdraw.\",\n        \"Lemma\": \"bank\",\n        \"Label\": 1\n    },\n    {\n        \"Sentence 1\": \"The chef prepared a delicious dish.\",\n        \"Sentence 2\": \"The satellite captured a dish image from space.\",\n        \"Lemma\": \"dish\",\n        \"Label\": 0\n    },\n    {\n        \"Sentence 1\": \"The coach gave a motivational speech before the game.\",\n        \"Sentence 2\": \"The coach broke down on the highway.\",\n        \"Lemma\": \"coach\",\n        \"Label\": 0\n    },\n    {\n        \"Sentence 1\": \"She dropped her ring on the floor.\",\n        \"Sentence 2\": \"The ring of the phone startled everyone.\",\n        \"Lemma\": \"ring\",\n        \"Label\": 0\n    }\n]\n\n# Convert to DataFrame so it can be used easily\ndf_balanced_manual = pd.DataFrame(balanced_test_samples)\ndf_balanced_manual\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T14:45:15.639245Z","iopub.execute_input":"2025-05-02T14:45:15.639728Z","iopub.status.idle":"2025-05-02T14:45:15.652410Z","shell.execute_reply.started":"2025-05-02T14:45:15.639707Z","shell.execute_reply":"2025-05-02T14:45:15.651848Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                          Sentence 1  \\\n0            The lawyer presented his case in court.   \n1               He closed the deal with a handshake.   \n2       He went to the bank to deposit his paycheck.   \n3                The chef prepared a delicious dish.   \n4  The coach gave a motivational speech before th...   \n5                 She dropped her ring on the floor.   \n\n                                          Sentence 2  Lemma  Label  \n0  The detective reviewed the case again to find ...   case      1  \n1            She found a great deal on shoes online.   deal      1  \n2  She visited the bank with her friend yesterday...   bank      1  \n3    The satellite captured a dish image from space.   dish      0  \n4               The coach broke down on the highway.  coach      0  \n5           The ring of the phone startled everyone.   ring      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence 1</th>\n      <th>Sentence 2</th>\n      <th>Lemma</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The lawyer presented his case in court.</td>\n      <td>The detective reviewed the case again to find ...</td>\n      <td>case</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>He closed the deal with a handshake.</td>\n      <td>She found a great deal on shoes online.</td>\n      <td>deal</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>He went to the bank to deposit his paycheck.</td>\n      <td>She visited the bank with her friend yesterday...</td>\n      <td>bank</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The chef prepared a delicious dish.</td>\n      <td>The satellite captured a dish image from space.</td>\n      <td>dish</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The coach gave a motivational speech before th...</td>\n      <td>The coach broke down on the highway.</td>\n      <td>coach</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>She dropped her ring on the floor.</td>\n      <td>The ring of the phone startled everyone.</td>\n      <td>ring</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# =============================\n# Train full data, test on sampled examples\n# =============================\n\ndef train_and_eval(data_path, definition_col, model_save_name):\n    # Step 1: Load and prepare training data\n    df = pd.read_csv(data_path).rename(columns={definition_col: \"Gloss\"})\n    df = df.dropna(subset=[\"Gloss\"])\n    df[\"Gloss\"] = df[\"Gloss\"].astype(str)\n    sentence_pairs = list(zip(df[\"Context Sentence\"], df[\"Gloss\"]))\n    labels = list(df[\"Label\"])\n\n    # Train/val split (90/10)\n    train_pairs, val_pairs, train_labels, val_labels = train_test_split(\n        sentence_pairs, labels, test_size=0.1, random_state=42\n    )\n\n    train_dataset = WSDSiameseDataset(train_pairs, train_labels, tokenizer)\n    val_dataset = WSDSiameseDataset(val_pairs, val_labels, tokenizer)\n\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n\n    # Step 2: Train the model\n    model.to(device)\n    scaler = GradScaler()\n    best_val_loss = float('inf')\n\n    for epoch in range(2):  # You can increase this\n        model.train()\n        total_loss = 0\n        for batch in tqdm(train_loader, desc=\"Training\"):\n            optimizer.zero_grad()\n            with autocast():\n                distances = model(\n                    batch[\"input_ids_1\"].to(device),\n                    batch[\"attention_mask_1\"].to(device),\n                    batch[\"input_ids_2\"].to(device),\n                    batch[\"attention_mask_2\"].to(device),\n                    batch[\"labels\"].to(device)\n                )\n                loss = loss_fn(distances, batch[\"labels\"].to(device))\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            total_loss += loss.item()\n\n        val_loss = evaluate(model, val_loader, loss_fn, device)\n        print(f\"Training Loss: {total_loss/len(train_loader):.5f}, Validation Loss: {val_loss:.5f}\")\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.bert.state_dict(), model_save_name)\n            print(f\"[INFO] Saved best model with val loss: {val_loss:.4f}\")\n\n    # Step 3: Evaluate on manually defined test set\n    print(\"\\n===== Manual Evaluation on Balanced Samples =====\")\n    test_pairs = list(zip(df_balanced_manual[\"Sentence 1\"], df_balanced_manual[\"Sentence 2\"]))\n    test_labels = list(df_balanced_manual[\"Label\"])\n\n    test_dataset = WSDSiameseDataset(test_pairs, test_labels, tokenizer)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n    distances, labels = predict(model, test_loader, device)\n\n    for i in range(len(df_balanced_manual)):\n        print(f\"\\n🔹 Example {i+1}\")\n        print(f\"Sentence 1: {df_balanced_manual.iloc[i]['Sentence 1']}\")\n        print(f\"Sentence 2: {df_balanced_manual.iloc[i]['Sentence 2']}\")\n        print(f\"Lemma: {df_balanced_manual.iloc[i]['Lemma']}\")\n        print(f\"True Label: {df_balanced_manual.iloc[i]['Label']} | Predicted Distance: {distances[i]:.4f}\")\n\n    acc, f1 = compute_accuracy_f1(distances, labels)\n    print(f\"\\n✅ Accuracy: {acc*100:.4f}%, F1 Score: {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T14:45:15.653158Z","iopub.execute_input":"2025-05-02T14:45:15.653795Z","iopub.status.idle":"2025-05-02T14:45:15.668463Z","shell.execute_reply.started":"2025-05-02T14:45:15.653776Z","shell.execute_reply":"2025-05-02T14:45:15.667835Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Run\ntrain_and_eval(\"/kaggle/input/datasets-contrastive/context_gloss_pairs_mixed.csv\", \"Synset Gloss Definition\", \"shared_weights_gloss.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T14:45:15.669188Z","iopub.execute_input":"2025-05-02T14:45:15.669428Z","iopub.status.idle":"2025-05-02T14:48:34.093226Z","shell.execute_reply.started":"2025-05-02T14:45:15.669412Z","shell.execute_reply":"2025-05-02T14:48:34.092482Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_109/904384342.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\nTraining:   0%|          | 0/2226 [00:00<?, ?it/s]/tmp/ipykernel_109/904384342.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nTraining: 100%|██████████| 2226/2226 [03:08<00:00, 11.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.00139, Validation Loss: 0.00002\n[INFO] Saved best model with val loss: 0.0000\n\n===== Manual Evaluation on Balanced Samples =====\n\n🔹 Example 1\nSentence 1: The lawyer presented his case in court.\nSentence 2: The detective reviewed the case again to find new clues.\nLemma: case\nTrue Label: 1 | Predicted Distance: 0.0187\n\n🔹 Example 2\nSentence 1: He closed the deal with a handshake.\nSentence 2: She found a great deal on shoes online.\nLemma: deal\nTrue Label: 1 | Predicted Distance: 0.0164\n\n🔹 Example 3\nSentence 1: He went to the bank to deposit his paycheck.\nSentence 2: She visited the bank with her friend yesterday to withdraw.\nLemma: bank\nTrue Label: 1 | Predicted Distance: 0.0126\n\n🔹 Example 4\nSentence 1: The chef prepared a delicious dish.\nSentence 2: The satellite captured a dish image from space.\nLemma: dish\nTrue Label: 0 | Predicted Distance: -0.9812\n\n🔹 Example 5\nSentence 1: The coach gave a motivational speech before the game.\nSentence 2: The coach broke down on the highway.\nLemma: coach\nTrue Label: 0 | Predicted Distance: -0.9834\n\n🔹 Example 6\nSentence 1: She dropped her ring on the floor.\nSentence 2: The ring of the phone startled everyone.\nLemma: ring\nTrue Label: 0 | Predicted Distance: -0.9657\n[1. 1. 1. 0. 0. 0.]\n[1 1 1 0 0 0]\n\n✅ Accuracy: 1.0000, F1 Score: 1.0000\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\nmodel = SiameseBERT(model_name=model_name)\nmodel.bert.load_state_dict(torch.load(\"/kaggle/working/shared_weights_gloss.pth\"))\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T14:48:34.094339Z","iopub.execute_input":"2025-05-02T14:48:34.094886Z","iopub.status.idle":"2025-05-02T14:48:34.457258Z","shell.execute_reply.started":"2025-05-02T14:48:34.094863Z","shell.execute_reply":"2025-05-02T14:48:34.456516Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_109/1207260939.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.bert.load_state_dict(torch.load(\"/kaggle/working/shared_weights_gloss.pth\"))\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"train_and_eval(\"/kaggle/input/datasets-contrastive/context_hypernym_pairs_mixed.csv\", \"Hypernym Gloss Definition\", \"shared_weights_gloss_hypernym.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T14:48:34.458213Z","iopub.execute_input":"2025-05-02T14:48:34.458487Z","iopub.status.idle":"2025-05-02T14:50:46.687444Z","shell.execute_reply.started":"2025-05-02T14:48:34.458464Z","shell.execute_reply":"2025-05-02T14:50:46.686511Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_109/904384342.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\nTraining:   0%|          | 0/1494 [00:00<?, ?it/s]/tmp/ipykernel_109/904384342.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nTraining: 100%|██████████| 1494/1494 [02:06<00:00, 11.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.00012, Validation Loss: 0.00012\n[INFO] Saved best model with val loss: 0.0001\n\n===== Manual Evaluation on Balanced Samples =====\n\n🔹 Example 1\nSentence 1: The lawyer presented his case in court.\nSentence 2: The detective reviewed the case again to find new clues.\nLemma: case\nTrue Label: 1 | Predicted Distance: 0.0031\n\n🔹 Example 2\nSentence 1: He closed the deal with a handshake.\nSentence 2: She found a great deal on shoes online.\nLemma: deal\nTrue Label: 1 | Predicted Distance: 0.0017\n\n🔹 Example 3\nSentence 1: He went to the bank to deposit his paycheck.\nSentence 2: She visited the bank with her friend yesterday to withdraw.\nLemma: bank\nTrue Label: 1 | Predicted Distance: 0.0017\n\n🔹 Example 4\nSentence 1: The chef prepared a delicious dish.\nSentence 2: The satellite captured a dish image from space.\nLemma: dish\nTrue Label: 0 | Predicted Distance: -0.9973\n\n🔹 Example 5\nSentence 1: The coach gave a motivational speech before the game.\nSentence 2: The coach broke down on the highway.\nLemma: coach\nTrue Label: 0 | Predicted Distance: -0.9981\n\n🔹 Example 6\nSentence 1: She dropped her ring on the floor.\nSentence 2: The ring of the phone startled everyone.\nLemma: ring\nTrue Label: 0 | Predicted Distance: -0.9940\n[1. 1. 1. 0. 0. 0.]\n[1 1 1 0 0 0]\n\n✅ Accuracy: 1.0000, F1 Score: 1.0000\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}