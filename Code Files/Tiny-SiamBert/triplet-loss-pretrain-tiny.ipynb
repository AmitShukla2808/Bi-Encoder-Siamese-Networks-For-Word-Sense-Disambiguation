{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11659747,"sourceType":"datasetVersion","datasetId":7317080},{"sourceId":370895,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":307026,"modelId":327508}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer, BertModel\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn.model_selection import train_test_split\n# import wic\nimport warnings\nfrom tqdm import tqdm\nfrom torch.cuda.amp import GradScaler, autocast\nfrom datasets import Dataset\nimport math\nimport pandas as pd\nimport numpy as np\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:52:40.690687Z","iopub.execute_input":"2025-05-03T14:52:40.690927Z","iopub.status.idle":"2025-05-03T14:53:05.365209Z","shell.execute_reply.started":"2025-05-03T14:52:40.690908Z","shell.execute_reply":"2025-05-03T14:53:05.364655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass WSDTripletDataset(Dataset):\n    def __init__(self, hf_dataset, tokenizer, max_length):\n        self.dataset = hf_dataset\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        data = self.dataset.iloc[idx]  # Use iloc for pandas DataFrame\n\n        def tokenize(text):\n            tokens = self.tokenizer(\n                text,\n                padding=\"max_length\",\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors=\"pt\"\n            )\n            return {\n                'input_ids': tokens['input_ids'].squeeze(0),\n                'attention_mask': tokens['attention_mask'].squeeze(0)\n            }\n\n        return {\n            'anchor': tokenize(data['anchor']),\n            'positive': tokenize(data['positive']),\n            'negative': tokenize(data['negative']),\n            'target_word': data['target_word']\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:53:05.366236Z","iopub.execute_input":"2025-05-03T14:53:05.366830Z","iopub.status.idle":"2025-05-03T14:53:05.372382Z","shell.execute_reply.started":"2025-05-03T14:53:05.366800Z","shell.execute_reply":"2025-05-03T14:53:05.371827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModel\n\nclass TripletBERT(nn.Module):\n    def __init__(self, model_name):\n        super(TripletBERT, self).__init__()\n        self.bert = AutoModel.from_pretrained(model_name)\n        self.hidden_size = self.bert.config.hidden_size\n        self.fc = nn.Linear(self.hidden_size, 256)\n\n    def get_embedding(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        proj = self.fc(cls_output)\n        return proj  \n\n    def forward(self, anchor, positive, negative):\n        anchor_embed = self.get_embedding(anchor[\"input_ids\"], anchor[\"attention_mask\"])\n        positive_embed = self.get_embedding(positive[\"input_ids\"], positive[\"attention_mask\"])\n        negative_embed = self.get_embedding(negative[\"input_ids\"], negative[\"attention_mask\"])\n        return anchor_embed, positive_embed, negative_embed\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:53:05.373338Z","iopub.execute_input":"2025-05-03T14:53:05.373643Z","iopub.status.idle":"2025-05-03T14:53:05.448383Z","shell.execute_reply.started":"2025-05-03T14:53:05.373617Z","shell.execute_reply":"2025-05-03T14:53:05.447774Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TripletLoss(nn.Module):\n    def __init__(self, margin=1.0):\n        super(TripletLoss, self).__init__()\n        self.loss_fn = nn.TripletMarginLoss(margin=margin, p=2)\n\n    def forward(self, anchor, positive, negative):\n        return self.loss_fn(anchor, positive, negative)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:53:05.449832Z","iopub.execute_input":"2025-05-03T14:53:05.450042Z","iopub.status.idle":"2025-05-03T14:53:05.465930Z","shell.execute_reply.started":"2025-05-03T14:53:05.450026Z","shell.execute_reply":"2025-05-03T14:53:05.465283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    return {\n        'anchor': {\n            'input_ids': torch.stack([item['anchor']['input_ids'] for item in batch]),\n            'attention_mask': torch.stack([item['anchor']['attention_mask'] for item in batch])\n        },\n        'positive': {\n            'input_ids': torch.stack([item['positive']['input_ids'] for item in batch]),\n            'attention_mask': torch.stack([item['positive']['attention_mask'] for item in batch])\n        },\n        'negative': {\n            'input_ids': torch.stack([item['negative']['input_ids'] for item in batch]),\n            'attention_mask': torch.stack([item['negative']['attention_mask'] for item in batch])\n        },\n        'target_word': [item['target_word'] for item in batch]\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:53:05.466613Z","iopub.execute_input":"2025-05-03T14:53:05.466812Z","iopub.status.idle":"2025-05-03T14:53:05.480582Z","shell.execute_reply.started":"2025-05-03T14:53:05.466796Z","shell.execute_reply":"2025-05-03T14:53:05.479882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport math\nfrom tqdm import tqdm\nfrom torch.cuda.amp import autocast, GradScaler\n\ndef train_triplet(model, trainloader, testloader, optimizer, loss_fn, device, num_epochs=5):\n    model.to(device)\n    scaler = GradScaler()\n    best_loss = math.inf\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0.0\n        progress_bar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n\n        for batch in progress_bar:\n            optimizer.zero_grad()\n\n            # Move to device\n            anchor_ids = batch[\"anchor\"][\"input_ids\"].to(device)\n            anchor_mask = batch[\"anchor\"][\"attention_mask\"].to(device)\n            pos_ids = batch[\"positive\"][\"input_ids\"].to(device)\n            pos_mask = batch[\"positive\"][\"attention_mask\"].to(device)\n            neg_ids = batch[\"negative\"][\"input_ids\"].to(device)\n            neg_mask = batch[\"negative\"][\"attention_mask\"].to(device)\n\n            with autocast():\n                anchor_embed = model.get_embedding(anchor_ids, anchor_mask)\n                pos_embed = model.get_embedding(pos_ids, pos_mask)\n                neg_embed = model.get_embedding(neg_ids, neg_mask)\n\n                loss = loss_fn(anchor_embed, pos_embed, neg_embed)\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            total_loss += loss.item()\n            progress_bar.set_postfix({\"Train Loss\": loss.item()})\n\n        avg_train_loss = total_loss / len(trainloader)\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for batch in testloader:\n                anchor_ids = batch[\"anchor\"][\"input_ids\"].to(device)\n                anchor_mask = batch[\"anchor\"][\"attention_mask\"].to(device)\n                pos_ids = batch[\"positive\"][\"input_ids\"].to(device)\n                pos_mask = batch[\"positive\"][\"attention_mask\"].to(device)\n                neg_ids = batch[\"negative\"][\"input_ids\"].to(device)\n                neg_mask = batch[\"negative\"][\"attention_mask\"].to(device)\n\n                anchor_embed = model.get_embedding(anchor_ids, anchor_mask)\n                pos_embed = model.get_embedding(pos_ids, pos_mask)\n                neg_embed = model.get_embedding(neg_ids, neg_mask)\n\n                loss = loss_fn(anchor_embed, pos_embed, neg_embed)\n                val_loss += loss.item()\n\n        avg_val_loss = val_loss / len(testloader)\n\n        # Save the best model based on validation loss\n        if avg_val_loss < best_loss:\n            best_loss = avg_val_loss\n            torch.save(model.bert.state_dict(), \"shared_weights_triplet_tiny.pth\")\n\n        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:53:05.481237Z","iopub.execute_input":"2025-05-03T14:53:05.481551Z","iopub.status.idle":"2025-05-03T14:53:05.496668Z","shell.execute_reply.started":"2025-05-03T14:53:05.481522Z","shell.execute_reply":"2025-05-03T14:53:05.496186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load and preprocess the triplet dataset\ndata_triplet = pd.read_csv(\"/kaggle/input/context-positive-negative/context_gloss_triplets_all_negatives.csv\")\ndata_triplet = data_triplet.rename(columns={\n    'Context Sentence (Anchor)': 'anchor',\n    'Gloss Definition (Positive)': 'positive',\n    'Gloss Definition (Negative)': 'negative'\n})\ndata_triplet['target_word'] = data_triplet['positive'].str.extract(r'^(\\w+)\\s*:')\n\ntriplet_df = data_triplet.dropna(subset=['anchor', 'positive', 'negative', 'target_word'])\ntriplet_df = triplet_df.astype(str)\n\ntrain_data, test_data = train_test_split(triplet_df, test_size=0.2, random_state=42)\ntrain_data = train_data.reset_index(drop=True)\ntest_data = test_data.reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:53:05.497230Z","iopub.execute_input":"2025-05-03T14:53:05.497417Z","iopub.status.idle":"2025-05-03T14:53:06.049193Z","shell.execute_reply.started":"2025-05-03T14:53:05.497402Z","shell.execute_reply":"2025-05-03T14:53:06.048457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_name = \"huawei-noah/TinyBERT_General_4L_312D\"\nmodel = TripletBERT(model_name=model_name)\nmodel.bert.load_state_dict(torch.load(\"/kaggle/input/gloss-hypernymy-tinybert-pretrained/pytorch/default/1/shared_weights_gloss_hypernym_tiny.pth\", map_location=device))\n\n# Updated optimizer and loss\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n\nloss_fn = TripletLoss(margin=5.0)  # Lower margin helps smoother convergence","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:53:06.049967Z","iopub.execute_input":"2025-05-03T14:53:06.050228Z","iopub.status.idle":"2025-05-03T14:53:12.094026Z","shell.execute_reply.started":"2025-05-03T14:53:06.050204Z","shell.execute_reply":"2025-05-03T14:53:12.093102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenizer and Dataloaders\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntrain_dataset = WSDTripletDataset(train_data, tokenizer, max_length=128)\ntest_dataset = WSDTripletDataset(test_data, tokenizer, max_length=128)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:53:12.095033Z","iopub.execute_input":"2025-05-03T14:53:12.095331Z","iopub.status.idle":"2025-05-03T14:53:12.633518Z","shell.execute_reply.started":"2025-05-03T14:53:12.095312Z","shell.execute_reply":"2025-05-03T14:53:12.632959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Model setup\n# model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\n# model = TripletBERT(model_name=model_name)\n# optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n# loss_fn = TripletLoss(margin=1)\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:53:12.635056Z","iopub.execute_input":"2025-05-03T14:53:12.635463Z","iopub.status.idle":"2025-05-03T14:53:12.638710Z","shell.execute_reply.started":"2025-05-03T14:53:12.635444Z","shell.execute_reply":"2025-05-03T14:53:12.637950Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model\ntrain_triplet(\n    model=model,\n    trainloader=train_loader,\n    testloader = test_loader,\n    optimizer=optimizer,\n    loss_fn=loss_fn,\n    device=device,\n    num_epochs=30\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:53:12.639489Z","iopub.execute_input":"2025-05-03T14:53:12.639731Z","iopub.status.idle":"2025-05-03T15:13:45.081762Z","shell.execute_reply.started":"2025-05-03T14:53:12.639714Z","shell.execute_reply":"2025-05-03T15:13:45.080998Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation\n","metadata":{}},{"cell_type":"code","source":"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# # Reinitialize model and load state_dict\n# model = TripletBERT(model_name=model_name).to(device)\n# model.load_state_dict(torch.load(\"/kaggle/input/triplet_tinybert_30_0.7m/pytorch/default/1/triplet_tinybert_25_0.7M.pth\", map_location=device))\n# model.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:13:51.603357Z","iopub.execute_input":"2025-05-03T15:13:51.603886Z","iopub.status.idle":"2025-05-03T15:13:51.607261Z","shell.execute_reply.started":"2025-05-03T15:13:51.603861Z","shell.execute_reply":"2025-05-03T15:13:51.606546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Example anchor and positive sentences\nanchor_sentence = \"The cat is on the mat.\"\npositive_sentence = \"A cat is sitting on a mat.\"\n\n# Tokenize the sentences\nanchor_tokens = tokenizer(anchor_sentence, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\npositive_tokens = tokenizer(positive_sentence, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n\n# Move tokens to the device\nanchor_tokens = {k: v.to(device) for k, v in anchor_tokens.items()}\npositive_tokens = {k: v.to(device) for k, v in positive_tokens.items()}\n\n# Get the embeddings from the model\nmodel.eval()  # Switch to evaluation mode\nwith torch.no_grad():\n    anchor_embed = model.get_embedding(anchor_tokens['input_ids'], anchor_tokens['attention_mask'])\n    positive_embed = model.get_embedding(positive_tokens['input_ids'], positive_tokens['attention_mask'])\n\n# Convert embeddings to numpy for cosine similarity calculation\nanchor_embed = anchor_embed.cpu().numpy()\npositive_embed = positive_embed.cpu().numpy()\n\n# Compute cosine similarity\ncosine_sim = cosine_similarity(anchor_embed, positive_embed)\nprint(f\"Cosine Similarity between anchor and positive sentence: {cosine_sim[0][0]:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:13:52.519848Z","iopub.execute_input":"2025-05-03T15:13:52.520113Z","iopub.status.idle":"2025-05-03T15:13:52.550639Z","shell.execute_reply.started":"2025-05-03T15:13:52.520093Z","shell.execute_reply":"2025-05-03T15:13:52.550092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example negative sentence\nnegative_sentence = \"A dog is barking outside.\"\n\n# Tokenize the negative sentence\nnegative_tokens = tokenizer(negative_sentence, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\nnegative_tokens = {k: v.to(device) for k, v in negative_tokens.items()}\n\nmodel.eval()\n# Get the negative embedding from the model\nwith torch.no_grad():\n    negative_embed = model.get_embedding(negative_tokens['input_ids'], negative_tokens['attention_mask'])\n\n# Convert to numpy for cosine similarity\nnegative_embed = negative_embed.cpu().numpy()\n\n# Compute cosine similarity for both anchor-positive and anchor-negative\ncosine_sim_pos = cosine_similarity(anchor_embed, positive_embed)\ncosine_sim_neg = cosine_similarity(anchor_embed, negative_embed)\n\nprint(f\"Cosine Similarity between anchor and positive sentence: {cosine_sim_pos[0][0]:.4f}\")\nprint(f\"Cosine Similarity between anchor and negative sentence: {cosine_sim_neg[0][0]:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:13:55.555483Z","iopub.execute_input":"2025-05-03T15:13:55.555776Z","iopub.status.idle":"2025-05-03T15:13:55.567702Z","shell.execute_reply.started":"2025-05-03T15:13:55.555756Z","shell.execute_reply":"2025-05-03T15:13:55.566994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}